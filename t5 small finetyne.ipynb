{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    DataCollatorForSeq2Seq\n)\nfrom rouge_score import rouge_scorer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T22:02:33.398236Z","iopub.execute_input":"2024-12-24T22:02:33.398620Z","iopub.status.idle":"2024-12-24T22:02:33.402762Z","shell.execute_reply.started":"2024-12-24T22:02:33.398581Z","shell.execute_reply":"2024-12-24T22:02:33.401882Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"# Отключение wandb\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Проверяем использование GPU\nprint(\"Using GPU:\", torch.cuda.is_available())\n\n# Шаг 1: Загрузка датасета\nprint(\"Загрузка датасета...\")\ndataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\nprint(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T22:02:33.407509Z","iopub.execute_input":"2024-12-24T22:02:33.407772Z","iopub.status.idle":"2024-12-24T22:02:34.854764Z","shell.execute_reply.started":"2024-12-24T22:02:33.407750Z","shell.execute_reply":"2024-12-24T22:02:34.853794Z"}},"outputs":[{"name":"stdout","text":"Using GPU: True\nЗагрузка датасета...\nDatasetDict({\n    train: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 287113\n    })\n    validation: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 13368\n    })\n    test: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 11490\n    })\n})\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"# Шаг 2: Токенизация данных\nprint(\"Токенизация данных...\")\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n\ndef preprocess_data(examples):\n    inputs = [\"summarize: \" + doc for doc in examples[\"article\"]]\n    model_inputs = tokenizer(inputs, max_length=128, padding=\"max_length\", truncation=True)  # Ограничение длины\n    labels = tokenizer(examples[\"highlights\"], max_length=32, padding=\"max_length\", truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset = dataset.map(preprocess_data, batched=True)\n\n# Подготовка уменьшенного набора данных для тестирования\n\ntrain_data = tokenized_dataset[\"train\"].select(range(len(tokenized_dataset[\"train\"]) // 2))  # Половина данных\nval_data = tokenized_dataset[\"validation\"].select(range(len(tokenized_dataset[\"validation\"]) // 2))\ntest_data = tokenized_dataset[\"test\"].select(range(len(tokenized_dataset[\"test\"]) // 2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T22:02:34.855770Z","iopub.execute_input":"2024-12-24T22:02:34.856076Z","iopub.status.idle":"2024-12-24T22:10:12.021881Z","shell.execute_reply.started":"2024-12-24T22:02:34.856051Z","shell.execute_reply":"2024-12-24T22:10:12.021207Z"}},"outputs":[{"name":"stdout","text":"Токенизация данных...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee7966a138e4db782dde6bf2cf11dac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffe70af8e2ce447e9461b9132695a591"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18ba61cbff944a619c361f919bea9de2"}},"metadata":{}},{"name":"stderr","text":"loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"868f99f3563c47acb94ceedcdf62bb65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1837a3b2ef3e41668ec9c9152afb516d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4cacb92715e4e4e98471efcd35763f8"}},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"# Шаг 3: Загрузка модели\nprint(\"Загрузка модели...\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T22:10:12.023750Z","iopub.execute_input":"2024-12-24T22:10:12.023973Z","iopub.status.idle":"2024-12-24T22:10:13.731488Z","shell.execute_reply.started":"2024-12-24T22:10:12.023954Z","shell.execute_reply":"2024-12-24T22:10:13.730407Z"}},"outputs":[{"name":"stdout","text":"Загрузка модели...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7377d95de3094e959ab2d96f99129adc"}},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\nModel config T5Config {\n  \"_name_or_path\": \"t5-small\",\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 2048,\n  \"d_kv\": 64,\n  \"d_model\": 512,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 6,\n  \"num_heads\": 8,\n  \"num_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"transformers_version\": \"4.44.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cde450882353485cb81dda50e1a826c5"}},"metadata":{}},{"name":"stderr","text":"loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\nGenerate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\nAll model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\nAll the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13a9a52b97bc406b981bd8425d829d5f"}},"metadata":{}},{"name":"stderr","text":"loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\nGenerate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"# Шаг 4: Настройка функции вычисления метрик\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    rouge1_scores = []\n    rouge2_scores = []\n    rougeL_scores = []\n\n    for pred, label in zip(decoded_preds, decoded_labels):\n        scores = scorer.score(label, pred)\n        rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n        rouge2_scores.append(scores[\"rouge2\"].fmeasure)\n        rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n\n    return {\n        \"rouge1\": sum(rouge1_scores) / len(rouge1_scores),\n        \"rouge2\": sum(rouge2_scores) / len(rouge2_scores),\n        \"rougeL\": sum(rougeL_scores) / len(rougeL_scores),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T22:10:13.733100Z","iopub.execute_input":"2024-12-24T22:10:13.733420Z","iopub.status.idle":"2024-12-24T22:10:13.740570Z","shell.execute_reply.started":"2024-12-24T22:10:13.733388Z","shell.execute_reply":"2024-12-24T22:10:13.739823Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"# Шаг 5: Настройка параметров обучения\nprint(\"Настройка параметров обучения...\")\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    learning_rate=1e-5,\n    per_device_train_batch_size=4,  # Небольшой размер батча\n    per_device_eval_batch_size=4,\n    num_train_epochs=7,  # Увеличенное количество эпох\n    gradient_accumulation_steps=8,  # Для имитации большего батча\n    predict_with_generate=True,\n    save_total_limit=2,\n    logging_steps=50,\n    log_level=\"info\",\n    fp16=True,  # Смешанная точность\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T22:10:13.741483Z","iopub.execute_input":"2024-12-24T22:10:13.741783Z","iopub.status.idle":"2024-12-24T22:10:13.785838Z","shell.execute_reply.started":"2024-12-24T22:10:13.741753Z","shell.execute_reply":"2024-12-24T22:10:13.785016Z"}},"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"Настройка параметров обучения...\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# Шаг 6: Создание тренера\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T22:10:13.786738Z","iopub.execute_input":"2024-12-24T22:10:13.787028Z","iopub.status.idle":"2024-12-24T22:10:13.896027Z","shell.execute_reply.started":"2024-12-24T22:10:13.786998Z","shell.execute_reply":"2024-12-24T22:10:13.895152Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nUsing auto half precision backend\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"# Шаг 7: Обучение\nprint(\"Начало обучения...\")\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T22:10:13.897045Z","iopub.execute_input":"2024-12-24T22:10:13.897339Z","iopub.status.idle":"2024-12-25T02:57:54.954111Z","shell.execute_reply.started":"2024-12-24T22:10:13.897315Z","shell.execute_reply":"2024-12-25T02:57:54.953201Z"}},"outputs":[{"name":"stdout","text":"Начало обучения...\n","output_type":"stream"},{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: article, id, highlights. If article, id, highlights are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 143,556\n  Num Epochs = 7\n  Instantaneous batch size per device = 4\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 8\n  Total optimization steps = 31,402\n  Number of trainable parameters = 60,506,624\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='31402' max='31402' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [31402/31402 4:47:40, Epoch 6/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2.749200</td>\n      <td>2.163796</td>\n      <td>0.315993</td>\n      <td>0.149637</td>\n      <td>0.269477</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2.705000</td>\n      <td>2.135216</td>\n      <td>0.316703</td>\n      <td>0.149180</td>\n      <td>0.270056</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.650600</td>\n      <td>2.126991</td>\n      <td>0.317847</td>\n      <td>0.150921</td>\n      <td>0.271172</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.622400</td>\n      <td>2.113619</td>\n      <td>0.317381</td>\n      <td>0.151159</td>\n      <td>0.270975</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.625600</td>\n      <td>2.103940</td>\n      <td>0.318482</td>\n      <td>0.152007</td>\n      <td>0.271794</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.625700</td>\n      <td>2.100536</td>\n      <td>0.318585</td>\n      <td>0.151775</td>\n      <td>0.271597</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.608600</td>\n      <td>2.096549</td>\n      <td>0.318511</td>\n      <td>0.151635</td>\n      <td>0.271466</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./results/checkpoint-500\nConfiguration saved in ./results/checkpoint-500/config.json\nConfiguration saved in ./results/checkpoint-500/generation_config.json\nModel weights saved in ./results/checkpoint-500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-500/spiece.model\nSaving model checkpoint to ./results/checkpoint-1000\nConfiguration saved in ./results/checkpoint-1000/config.json\nConfiguration saved in ./results/checkpoint-1000/generation_config.json\nModel weights saved in ./results/checkpoint-1000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-1000/spiece.model\nDeleting older checkpoint [results/checkpoint-375] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-1500\nConfiguration saved in ./results/checkpoint-1500/config.json\nConfiguration saved in ./results/checkpoint-1500/generation_config.json\nModel weights saved in ./results/checkpoint-1500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-1500/spiece.model\nDeleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-2000\nConfiguration saved in ./results/checkpoint-2000/config.json\nConfiguration saved in ./results/checkpoint-2000/generation_config.json\nModel weights saved in ./results/checkpoint-2000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-2000/spiece.model\nDeleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-2500\nConfiguration saved in ./results/checkpoint-2500/config.json\nConfiguration saved in ./results/checkpoint-2500/generation_config.json\nModel weights saved in ./results/checkpoint-2500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-2500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-2500/spiece.model\nDeleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-3000\nConfiguration saved in ./results/checkpoint-3000/config.json\nConfiguration saved in ./results/checkpoint-3000/generation_config.json\nModel weights saved in ./results/checkpoint-3000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-3000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-3000/spiece.model\nDeleting older checkpoint [results/checkpoint-2000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-3500\nConfiguration saved in ./results/checkpoint-3500/config.json\nConfiguration saved in ./results/checkpoint-3500/generation_config.json\nModel weights saved in ./results/checkpoint-3500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-3500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-3500/spiece.model\nDeleting older checkpoint [results/checkpoint-2500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-4000\nConfiguration saved in ./results/checkpoint-4000/config.json\nConfiguration saved in ./results/checkpoint-4000/generation_config.json\nModel weights saved in ./results/checkpoint-4000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-4000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-4000/spiece.model\nDeleting older checkpoint [results/checkpoint-3000] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: article, id, highlights. If article, id, highlights are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 6684\n  Batch size = 4\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSaving model checkpoint to ./results/checkpoint-4500\nConfiguration saved in ./results/checkpoint-4500/config.json\nConfiguration saved in ./results/checkpoint-4500/generation_config.json\nModel weights saved in ./results/checkpoint-4500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-4500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-4500/spiece.model\nDeleting older checkpoint [results/checkpoint-3500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-5000\nConfiguration saved in ./results/checkpoint-5000/config.json\nConfiguration saved in ./results/checkpoint-5000/generation_config.json\nModel weights saved in ./results/checkpoint-5000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-5000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-5000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-5000/spiece.model\nDeleting older checkpoint [results/checkpoint-4000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-5500\nConfiguration saved in ./results/checkpoint-5500/config.json\nConfiguration saved in ./results/checkpoint-5500/generation_config.json\nModel weights saved in ./results/checkpoint-5500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-5500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-5500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-5500/spiece.model\nDeleting older checkpoint [results/checkpoint-4500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-6000\nConfiguration saved in ./results/checkpoint-6000/config.json\nConfiguration saved in ./results/checkpoint-6000/generation_config.json\nModel weights saved in ./results/checkpoint-6000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-6000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-6000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-6000/spiece.model\nDeleting older checkpoint [results/checkpoint-5000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-6500\nConfiguration saved in ./results/checkpoint-6500/config.json\nConfiguration saved in ./results/checkpoint-6500/generation_config.json\nModel weights saved in ./results/checkpoint-6500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-6500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-6500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-6500/spiece.model\nDeleting older checkpoint [results/checkpoint-5500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-7000\nConfiguration saved in ./results/checkpoint-7000/config.json\nConfiguration saved in ./results/checkpoint-7000/generation_config.json\nModel weights saved in ./results/checkpoint-7000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-7000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-7000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-7000/spiece.model\nDeleting older checkpoint [results/checkpoint-6000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-7500\nConfiguration saved in ./results/checkpoint-7500/config.json\nConfiguration saved in ./results/checkpoint-7500/generation_config.json\nModel weights saved in ./results/checkpoint-7500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-7500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-7500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-7500/spiece.model\nDeleting older checkpoint [results/checkpoint-6500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-8000\nConfiguration saved in ./results/checkpoint-8000/config.json\nConfiguration saved in ./results/checkpoint-8000/generation_config.json\nModel weights saved in ./results/checkpoint-8000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-8000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-8000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-8000/spiece.model\nDeleting older checkpoint [results/checkpoint-7000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-8500\nConfiguration saved in ./results/checkpoint-8500/config.json\nConfiguration saved in ./results/checkpoint-8500/generation_config.json\nModel weights saved in ./results/checkpoint-8500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-8500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-8500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-8500/spiece.model\nDeleting older checkpoint [results/checkpoint-7500] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: article, id, highlights. If article, id, highlights are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 6684\n  Batch size = 4\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSaving model checkpoint to ./results/checkpoint-9000\nConfiguration saved in ./results/checkpoint-9000/config.json\nConfiguration saved in ./results/checkpoint-9000/generation_config.json\nModel weights saved in ./results/checkpoint-9000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-9000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-9000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-9000/spiece.model\nDeleting older checkpoint [results/checkpoint-8000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-9500\nConfiguration saved in ./results/checkpoint-9500/config.json\nConfiguration saved in ./results/checkpoint-9500/generation_config.json\nModel weights saved in ./results/checkpoint-9500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-9500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-9500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-9500/spiece.model\nDeleting older checkpoint [results/checkpoint-8500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-10000\nConfiguration saved in ./results/checkpoint-10000/config.json\nConfiguration saved in ./results/checkpoint-10000/generation_config.json\nModel weights saved in ./results/checkpoint-10000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-10000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-10000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-10000/spiece.model\nDeleting older checkpoint [results/checkpoint-9000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-10500\nConfiguration saved in ./results/checkpoint-10500/config.json\nConfiguration saved in ./results/checkpoint-10500/generation_config.json\nModel weights saved in ./results/checkpoint-10500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-10500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-10500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-10500/spiece.model\nDeleting older checkpoint [results/checkpoint-9500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-11000\nConfiguration saved in ./results/checkpoint-11000/config.json\nConfiguration saved in ./results/checkpoint-11000/generation_config.json\nModel weights saved in ./results/checkpoint-11000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-11000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-11000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-11000/spiece.model\nDeleting older checkpoint [results/checkpoint-10000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-11500\nConfiguration saved in ./results/checkpoint-11500/config.json\nConfiguration saved in ./results/checkpoint-11500/generation_config.json\nModel weights saved in ./results/checkpoint-11500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-11500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-11500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-11500/spiece.model\nDeleting older checkpoint [results/checkpoint-10500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-12000\nConfiguration saved in ./results/checkpoint-12000/config.json\nConfiguration saved in ./results/checkpoint-12000/generation_config.json\nModel weights saved in ./results/checkpoint-12000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-12000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-12000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-12000/spiece.model\nDeleting older checkpoint [results/checkpoint-11000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-12500\nConfiguration saved in ./results/checkpoint-12500/config.json\nConfiguration saved in ./results/checkpoint-12500/generation_config.json\nModel weights saved in ./results/checkpoint-12500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-12500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-12500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-12500/spiece.model\nDeleting older checkpoint [results/checkpoint-11500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-13000\nConfiguration saved in ./results/checkpoint-13000/config.json\nConfiguration saved in ./results/checkpoint-13000/generation_config.json\nModel weights saved in ./results/checkpoint-13000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-13000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-13000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-13000/spiece.model\nDeleting older checkpoint [results/checkpoint-12000] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: article, id, highlights. If article, id, highlights are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 6684\n  Batch size = 4\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSaving model checkpoint to ./results/checkpoint-13500\nConfiguration saved in ./results/checkpoint-13500/config.json\nConfiguration saved in ./results/checkpoint-13500/generation_config.json\nModel weights saved in ./results/checkpoint-13500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-13500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-13500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-13500/spiece.model\nDeleting older checkpoint [results/checkpoint-12500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-14000\nConfiguration saved in ./results/checkpoint-14000/config.json\nConfiguration saved in ./results/checkpoint-14000/generation_config.json\nModel weights saved in ./results/checkpoint-14000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-14000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-14000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-14000/spiece.model\nDeleting older checkpoint [results/checkpoint-13000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-14500\nConfiguration saved in ./results/checkpoint-14500/config.json\nConfiguration saved in ./results/checkpoint-14500/generation_config.json\nModel weights saved in ./results/checkpoint-14500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-14500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-14500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-14500/spiece.model\nDeleting older checkpoint [results/checkpoint-13500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-15000\nConfiguration saved in ./results/checkpoint-15000/config.json\nConfiguration saved in ./results/checkpoint-15000/generation_config.json\nModel weights saved in ./results/checkpoint-15000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-15000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-15000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-15000/spiece.model\nDeleting older checkpoint [results/checkpoint-14000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-15500\nConfiguration saved in ./results/checkpoint-15500/config.json\nConfiguration saved in ./results/checkpoint-15500/generation_config.json\nModel weights saved in ./results/checkpoint-15500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-15500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-15500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-15500/spiece.model\nDeleting older checkpoint [results/checkpoint-14500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-16000\nConfiguration saved in ./results/checkpoint-16000/config.json\nConfiguration saved in ./results/checkpoint-16000/generation_config.json\nModel weights saved in ./results/checkpoint-16000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-16000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-16000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-16000/spiece.model\nDeleting older checkpoint [results/checkpoint-15000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-16500\nConfiguration saved in ./results/checkpoint-16500/config.json\nConfiguration saved in ./results/checkpoint-16500/generation_config.json\nModel weights saved in ./results/checkpoint-16500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-16500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-16500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-16500/spiece.model\nDeleting older checkpoint [results/checkpoint-15500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-17000\nConfiguration saved in ./results/checkpoint-17000/config.json\nConfiguration saved in ./results/checkpoint-17000/generation_config.json\nModel weights saved in ./results/checkpoint-17000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-17000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-17000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-17000/spiece.model\nDeleting older checkpoint [results/checkpoint-16000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-17500\nConfiguration saved in ./results/checkpoint-17500/config.json\nConfiguration saved in ./results/checkpoint-17500/generation_config.json\nModel weights saved in ./results/checkpoint-17500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-17500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-17500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-17500/spiece.model\nDeleting older checkpoint [results/checkpoint-16500] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: article, id, highlights. If article, id, highlights are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 6684\n  Batch size = 4\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSaving model checkpoint to ./results/checkpoint-18000\nConfiguration saved in ./results/checkpoint-18000/config.json\nConfiguration saved in ./results/checkpoint-18000/generation_config.json\nModel weights saved in ./results/checkpoint-18000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-18000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-18000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-18000/spiece.model\nDeleting older checkpoint [results/checkpoint-17000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-18500\nConfiguration saved in ./results/checkpoint-18500/config.json\nConfiguration saved in ./results/checkpoint-18500/generation_config.json\nModel weights saved in ./results/checkpoint-18500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-18500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-18500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-18500/spiece.model\nDeleting older checkpoint [results/checkpoint-17500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-19000\nConfiguration saved in ./results/checkpoint-19000/config.json\nConfiguration saved in ./results/checkpoint-19000/generation_config.json\nModel weights saved in ./results/checkpoint-19000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-19000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-19000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-19000/spiece.model\nDeleting older checkpoint [results/checkpoint-18000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-19500\nConfiguration saved in ./results/checkpoint-19500/config.json\nConfiguration saved in ./results/checkpoint-19500/generation_config.json\nModel weights saved in ./results/checkpoint-19500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-19500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-19500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-19500/spiece.model\nDeleting older checkpoint [results/checkpoint-18500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-20000\nConfiguration saved in ./results/checkpoint-20000/config.json\nConfiguration saved in ./results/checkpoint-20000/generation_config.json\nModel weights saved in ./results/checkpoint-20000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-20000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-20000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-20000/spiece.model\nDeleting older checkpoint [results/checkpoint-19000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-20500\nConfiguration saved in ./results/checkpoint-20500/config.json\nConfiguration saved in ./results/checkpoint-20500/generation_config.json\nModel weights saved in ./results/checkpoint-20500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-20500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-20500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-20500/spiece.model\nDeleting older checkpoint [results/checkpoint-19500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-21000\nConfiguration saved in ./results/checkpoint-21000/config.json\nConfiguration saved in ./results/checkpoint-21000/generation_config.json\nModel weights saved in ./results/checkpoint-21000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-21000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-21000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-21000/spiece.model\nDeleting older checkpoint [results/checkpoint-20000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-21500\nConfiguration saved in ./results/checkpoint-21500/config.json\nConfiguration saved in ./results/checkpoint-21500/generation_config.json\nModel weights saved in ./results/checkpoint-21500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-21500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-21500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-21500/spiece.model\nDeleting older checkpoint [results/checkpoint-20500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-22000\nConfiguration saved in ./results/checkpoint-22000/config.json\nConfiguration saved in ./results/checkpoint-22000/generation_config.json\nModel weights saved in ./results/checkpoint-22000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-22000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-22000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-22000/spiece.model\nDeleting older checkpoint [results/checkpoint-21000] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: article, id, highlights. If article, id, highlights are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 6684\n  Batch size = 4\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSaving model checkpoint to ./results/checkpoint-22500\nConfiguration saved in ./results/checkpoint-22500/config.json\nConfiguration saved in ./results/checkpoint-22500/generation_config.json\nModel weights saved in ./results/checkpoint-22500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-22500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-22500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-22500/spiece.model\nDeleting older checkpoint [results/checkpoint-21500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-23000\nConfiguration saved in ./results/checkpoint-23000/config.json\nConfiguration saved in ./results/checkpoint-23000/generation_config.json\nModel weights saved in ./results/checkpoint-23000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-23000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-23000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-23000/spiece.model\nDeleting older checkpoint [results/checkpoint-22000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-23500\nConfiguration saved in ./results/checkpoint-23500/config.json\nConfiguration saved in ./results/checkpoint-23500/generation_config.json\nModel weights saved in ./results/checkpoint-23500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-23500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-23500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-23500/spiece.model\nDeleting older checkpoint [results/checkpoint-22500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-24000\nConfiguration saved in ./results/checkpoint-24000/config.json\nConfiguration saved in ./results/checkpoint-24000/generation_config.json\nModel weights saved in ./results/checkpoint-24000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-24000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-24000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-24000/spiece.model\nDeleting older checkpoint [results/checkpoint-23000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-24500\nConfiguration saved in ./results/checkpoint-24500/config.json\nConfiguration saved in ./results/checkpoint-24500/generation_config.json\nModel weights saved in ./results/checkpoint-24500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-24500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-24500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-24500/spiece.model\nDeleting older checkpoint [results/checkpoint-23500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-25000\nConfiguration saved in ./results/checkpoint-25000/config.json\nConfiguration saved in ./results/checkpoint-25000/generation_config.json\nModel weights saved in ./results/checkpoint-25000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-25000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-25000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-25000/spiece.model\nDeleting older checkpoint [results/checkpoint-24000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-25500\nConfiguration saved in ./results/checkpoint-25500/config.json\nConfiguration saved in ./results/checkpoint-25500/generation_config.json\nModel weights saved in ./results/checkpoint-25500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-25500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-25500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-25500/spiece.model\nDeleting older checkpoint [results/checkpoint-24500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-26000\nConfiguration saved in ./results/checkpoint-26000/config.json\nConfiguration saved in ./results/checkpoint-26000/generation_config.json\nModel weights saved in ./results/checkpoint-26000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-26000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-26000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-26000/spiece.model\nDeleting older checkpoint [results/checkpoint-25000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-26500\nConfiguration saved in ./results/checkpoint-26500/config.json\nConfiguration saved in ./results/checkpoint-26500/generation_config.json\nModel weights saved in ./results/checkpoint-26500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-26500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-26500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-26500/spiece.model\nDeleting older checkpoint [results/checkpoint-25500] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: article, id, highlights. If article, id, highlights are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 6684\n  Batch size = 4\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSaving model checkpoint to ./results/checkpoint-27000\nConfiguration saved in ./results/checkpoint-27000/config.json\nConfiguration saved in ./results/checkpoint-27000/generation_config.json\nModel weights saved in ./results/checkpoint-27000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-27000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-27000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-27000/spiece.model\nDeleting older checkpoint [results/checkpoint-26000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-27500\nConfiguration saved in ./results/checkpoint-27500/config.json\nConfiguration saved in ./results/checkpoint-27500/generation_config.json\nModel weights saved in ./results/checkpoint-27500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-27500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-27500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-27500/spiece.model\nDeleting older checkpoint [results/checkpoint-26500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-28000\nConfiguration saved in ./results/checkpoint-28000/config.json\nConfiguration saved in ./results/checkpoint-28000/generation_config.json\nModel weights saved in ./results/checkpoint-28000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-28000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-28000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-28000/spiece.model\nDeleting older checkpoint [results/checkpoint-27000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-28500\nConfiguration saved in ./results/checkpoint-28500/config.json\nConfiguration saved in ./results/checkpoint-28500/generation_config.json\nModel weights saved in ./results/checkpoint-28500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-28500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-28500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-28500/spiece.model\nDeleting older checkpoint [results/checkpoint-27500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-29000\nConfiguration saved in ./results/checkpoint-29000/config.json\nConfiguration saved in ./results/checkpoint-29000/generation_config.json\nModel weights saved in ./results/checkpoint-29000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-29000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-29000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-29000/spiece.model\nDeleting older checkpoint [results/checkpoint-28000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-29500\nConfiguration saved in ./results/checkpoint-29500/config.json\nConfiguration saved in ./results/checkpoint-29500/generation_config.json\nModel weights saved in ./results/checkpoint-29500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-29500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-29500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-29500/spiece.model\nDeleting older checkpoint [results/checkpoint-28500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-30000\nConfiguration saved in ./results/checkpoint-30000/config.json\nConfiguration saved in ./results/checkpoint-30000/generation_config.json\nModel weights saved in ./results/checkpoint-30000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-30000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-30000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-30000/spiece.model\nDeleting older checkpoint [results/checkpoint-29000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-30500\nConfiguration saved in ./results/checkpoint-30500/config.json\nConfiguration saved in ./results/checkpoint-30500/generation_config.json\nModel weights saved in ./results/checkpoint-30500/model.safetensors\ntokenizer config file saved in ./results/checkpoint-30500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-30500/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-30500/spiece.model\nDeleting older checkpoint [results/checkpoint-29500] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-31000\nConfiguration saved in ./results/checkpoint-31000/config.json\nConfiguration saved in ./results/checkpoint-31000/generation_config.json\nModel weights saved in ./results/checkpoint-31000/model.safetensors\ntokenizer config file saved in ./results/checkpoint-31000/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-31000/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-31000/spiece.model\nDeleting older checkpoint [results/checkpoint-30000] due to args.save_total_limit\nSaving model checkpoint to ./results/checkpoint-31402\nConfiguration saved in ./results/checkpoint-31402/config.json\nConfiguration saved in ./results/checkpoint-31402/generation_config.json\nModel weights saved in ./results/checkpoint-31402/model.safetensors\ntokenizer config file saved in ./results/checkpoint-31402/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-31402/special_tokens_map.json\nCopy vocab file to ./results/checkpoint-31402/spiece.model\nDeleting older checkpoint [results/checkpoint-30500] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: article, id, highlights. If article, id, highlights are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 6684\n  Batch size = 4\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=31402, training_loss=2.67335812453745, metrics={'train_runtime': 17260.6821, 'train_samples_per_second': 58.219, 'train_steps_per_second': 1.819, 'total_flos': 3.400002599858995e+16, 'train_loss': 2.67335812453745, 'epoch': 6.999804954164229})"},"metadata":{}}],"execution_count":65},{"cell_type":"code","source":"# Шаг 8: Оценка на тестовой выборке\nprint(\"Оценка на тестовой выборке...\")\nmetrics = trainer.evaluate(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T02:57:54.955861Z","iopub.execute_input":"2024-12-25T02:57:54.956102Z","iopub.status.idle":"2024-12-25T03:04:05.081631Z","shell.execute_reply.started":"2024-12-25T02:57:54.956081Z","shell.execute_reply":"2024-12-25T03:04:05.080745Z"}},"outputs":[{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: article, id, highlights. If article, id, highlights are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 5745\n  Batch size = 4\n","output_type":"stream"},{"name":"stdout","text":"Оценка на тестовой выборке...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1437' max='1437' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1437/1437 06:06]\n    </div>\n    "},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"# Шаг 9: Вывод метрик\ndef print_metrics(metrics):\n    print(\"Evaluation Metrics:\")\n    print(f\"ROUGE-1: {metrics['eval_rouge1']:.4f}\")\n    print(f\"ROUGE-2: {metrics['eval_rouge2']:.4f}\")\n    print(f\"ROUGE-L: {metrics['eval_rougeL']:.4f}\")\n    print(f\"Loss: {metrics['eval_loss']:.4f}\")\n    print(f\"Runtime (s): {metrics['eval_runtime']:.4f}\")\n    print(f\"Samples per Second: {metrics['eval_samples_per_second']:.2f}\")\n    print(f\"Steps per Second: {metrics['eval_steps_per_second']:.2f}\")\n\n# Вызов функции\nprint_metrics(metrics)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T03:04:05.082724Z","iopub.execute_input":"2024-12-25T03:04:05.083027Z","iopub.status.idle":"2024-12-25T03:04:05.089706Z","shell.execute_reply.started":"2024-12-25T03:04:05.082998Z","shell.execute_reply":"2024-12-25T03:04:05.088838Z"}},"outputs":[{"name":"stdout","text":"Evaluation Metrics:\nROUGE-1: 0.3173\nROUGE-2: 0.1514\nROUGE-L: 0.2710\nLoss: 2.1118\nRuntime (s): 370.1155\nSamples per Second: 15.52\nSteps per Second: 3.88\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}